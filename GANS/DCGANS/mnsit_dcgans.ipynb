{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "t_rugqeam-2p",
        "outputId": "79fb1b57-362d-4617-f43f-d5990afe9909"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# Imports and setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Initialize W&B project\n",
        "wandb.init(project=\"mnist_dcgan\", config={\n",
        "    \"dataset\": \"MNIST\",\n",
        "    \"framework\": \"PyTorch\",\n",
        "    \"model\": \"DCGAN\"\n",
        "})\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define preprocessing for MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download MNIST training dataset\n",
        "dataset = datasets.MNIST(root='./data',\n",
        "                         train=True,\n",
        "                         download=True,\n",
        "                         transform=transform)\n",
        "\n",
        "# Create DataLoader with batch size and shuffle\n",
        "batch_size = 128\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Get one batch to check shapes\n",
        "real_batch = next(iter(dataloader))\n",
        "images, labels = real_batch\n",
        "\n",
        "# Print shape of batch to verify\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "# Undo normalization for visual\n",
        "images = images * 0.5 + 0.5\n",
        "plt.figure(figsize=(8, 8))\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(images[i].squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D48TLnVtYR7"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "lr = 0.0002\n",
        "img_ch = 1\n",
        "img_size = 28\n",
        "Beta1 = 0.5\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, img_ch):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Starting size will be 7x7 after first layer\n",
        "        self.init_size = 7\n",
        "\n",
        "        # Linear layer to expand latent vector\n",
        "        self.fc = nn.Linear(latent_dim, 128 * self.init_size ** 2)\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "          nn.BatchNorm2d(128),\n",
        "\n",
        "          # Upsample to 14x14\n",
        "          nn.Upsample(scale_factor=2),\n",
        "          nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.LeakyReLU(inplace=True),\n",
        "\n",
        "          # Upsample to 28x28\n",
        "          nn.Upsample(scale_factor=2),\n",
        "          nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.LeakyReLU(inplace=True),\n",
        "\n",
        "          # Output layer\n",
        "          nn.Conv2d(64, img_ch, 3, stride=1, padding=1),\n",
        "          nn.Tanh()\n",
        "          # Output range [-1, 1]\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        # z shape: (batch_size, latent_dim)\n",
        "        out = self.fc(z)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8827e901"
      },
      "outputs": [],
      "source": [
        "# Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_ch):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Input: 1x28x28\n",
        "            # 14x14\n",
        "            nn.Conv2d(img_ch, 64, 4, stride=2, padding= 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # 7x7\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # 3x3\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        self.adv_layer = nn.Sequential(\n",
        "        # Flatten and output single value\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 3 * 3, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        out = self.model(img)\n",
        "        validity = self.adv_layer(out)\n",
        "        return validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4284b9e6"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "# Weight initialization (important for GANs)\n",
        "def weights_init(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find('Conv') != -1:\n",
        "    nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "  elif classname.find('BatchNorm') != -1:\n",
        "    nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "    nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "def train_dcgan(dataloader, num_epochs, batch_size):\n",
        "\n",
        "  # Initialize models\n",
        "  generator = Generator(latent_dim, img_ch).to(device)\n",
        "  discriminator = Discriminator(img_ch).to(device)\n",
        "\n",
        "  # Apply weight initialization\n",
        "  generator.apply(weights_init)\n",
        "  discriminator.apply(weights_init)\n",
        "  # Loss function\n",
        "  bce_loss = nn.BCELoss()\n",
        "  # Optimizers\n",
        "  optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(Beta1, 0.999))\n",
        "  optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(Beta1, 0.999))\n",
        "  # update WandB\n",
        "  wandb.config.update({\n",
        "      \"latent_dim\": latent_dim,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"learning_rate\": lr,\n",
        "      \"num_epochs\": num_epochs,\n",
        "      \"architecture\": \"Baseline DCGAN\"\n",
        "      })\n",
        "  # wandb.watch(generator)\n",
        "  # wandb.watch(discriminator)\n",
        "\n",
        "  # Fixed noise for visualization\n",
        "  fixed_noise = torch.randn(64, latent_dim).to(device)\n",
        "  print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
        "  print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (real_imgs,_) in enumerate(dataloader):\n",
        "      curr_batch_size = real_imgs.size(0) # Get current batch size\n",
        "      real_imgs = real_imgs.to(device)\n",
        "\n",
        "      # Labels for real and fake images, use curr_batch_size\n",
        "      real_labels = torch.ones(curr_batch_size, 1).to(device)\n",
        "      fake_labels = torch.zeros(curr_batch_size, 1).to(device)\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "\n",
        "      optimizer_D.zero_grad()\n",
        "\n",
        "      # Loss on real images\n",
        "      real_out = discriminator(real_imgs)\n",
        "      d_real_loss = bce_loss(real_out, real_labels)\n",
        "\n",
        "      # Generate fake images\n",
        "      z = torch.randn(curr_batch_size, latent_dim).to(device) # Use curr_batch_size for fake images\n",
        "      fake_imgs = generator(z)\n",
        "\n",
        "      # Loss on fake images\n",
        "      fake_out = discriminator(fake_imgs.detach())\n",
        "      d_fake_loss = bce_loss(fake_out, fake_labels)\n",
        "\n",
        "      # Total discriminator loss\n",
        "      d_loss = d_real_loss + d_fake_loss\n",
        "      d_loss.backward()\n",
        "      optimizer_D.step()\n",
        "\n",
        "    # -----------------\n",
        "    #  Train Generator\n",
        "    # -----------------\n",
        "    optimizer_G.zero_grad()\n",
        "\n",
        "    # Generate fake images\n",
        "    z = torch.randn(curr_batch_size, latent_dim).to(device) # Use curr_batch_size for fake images\n",
        "    fake_imgs = generator(z)\n",
        "\n",
        "    # Loss on fake images\n",
        "    fake_out = discriminator(fake_imgs)\n",
        "    g_loss = bce_loss(fake_out, real_labels)\n",
        "\n",
        "    # Generator tries to fool discriminator\n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    # ---------------------\n",
        "    #  Log metrics\n",
        "    # ---------------------\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print(f\"[Epoch {epoch}/{num_epochs}],\"\n",
        "      f\"[Batch {i}/{len(dataloader)}],\"\n",
        "      f\"D_loss: {d_loss.item():.4f},\"\n",
        "      f\"G_loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Log to WandB at end of epoch\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"d_loss\": d_loss.item(),\n",
        "        \"g_loss\": g_loss.item(),\n",
        "        \"d_real_acc\": (real_out >= 0.5).float().mean().item(), # Corrected accuracy calculation\n",
        "        \"d_fake_acc\": (fake_out < 0.5).float().mean().item() # Corrected accuracy calculation\n",
        "        })\n",
        "\n",
        "    # Generate and log images every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "      with torch.no_grad():\n",
        "        fake_imgs = generator(fixed_noise).detach().cpu()\n",
        "\n",
        "        # Denormalize images from [-1, 1] to [0, 1]\n",
        "        fake_imgs = (fake_imgs + 1) / 2\n",
        "        # Create grid of images\n",
        "        grid = torchvision.utils.make_grid(fake_imgs,\n",
        "                                           nrow=8,\n",
        "                                           padding=2,\n",
        "                                           normalize=False)\n",
        "        # Log to WandB\n",
        "        wandb.log({\"generated_images\": wandb.Image(grid)})\n",
        "        save_image(fake_imgs[:64],f'generated_epoch_{epoch}.png', nrow=8, padding=2, normalize=False)\n",
        "\n",
        "# Save final models\n",
        "  torch.save(generator.state_dict(), \"generator.pth\")\n",
        "  torch.save(discriminator.state_dict(), \"discriminator.pth\")\n",
        "  wandb.save(\"generator.pth\")\n",
        "  wandb.save(\"discriminator.pth\")\n",
        "\n",
        "  return generator, discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b19ae8c8",
        "outputId": "39a22089-a0e8-4554-d363-22586a89fe40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator parameters: 856,065\n",
            "Discriminator parameters: 659,905\n"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "# if __name__ == \"__main__\":\n",
        "  # Train the DCGAN\n",
        "generator, discriminator = train_dcgan(dataloader, epochs, batch_size)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14672258"
      },
      "outputs": [],
      "source": [
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXEgzTgUa2v2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
